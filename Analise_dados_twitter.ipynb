{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analise_dados_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoeN3e8_Gzk"
      },
      "source": [
        "# Análise de Dados do Twitter em Tempo Real com Python e Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRenfVnJCU8B"
      },
      "source": [
        "***Atenção***\n",
        "\n",
        "Utilize Java JDK 1.8 ou 11 e Apache Spark 2.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqSSGtwGDDMZ"
      },
      "source": [
        "## Instalação do Spark no Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn",
        "outputId": "e6f8b852-ca1e-4e0a-c1db-de14d49e0650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5-lspH_N8B"
      },
      "source": [
        "## Configuração de variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz",
        "outputId": "e7d85e7c-5240-48e6-c47f-6c07b5f11323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\t       spark-2.4.2-bin-hadoop2.7\n",
            "spark-2.3.1-bin-hadoop2.7      spark-2.4.2-bin-hadoop2.7.tgz\n",
            "spark-2.3.1-bin-hadoop2.7.tgz  spark-2.4.2-bin-hadoop2.7.tgz.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "## Spark Streaming Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiScZwiND4ul"
      },
      "source": [
        "# pacotes necessário\n",
        "#!pip install requests_oauthlib\n",
        "#!pip install twython\n",
        "#!pip install nltk\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SE-5v6uEY1Y"
      },
      "source": [
        "# Módulos usados\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark import SparkContext\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from operator import add\n",
        "from time import gmtime, strftime\n",
        "import requests_oauthlib\n",
        "import requests\n",
        "import time\n",
        "import string\n",
        "import ast\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNtvkuqFmgM"
      },
      "source": [
        "# Pacote NLTK\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.util import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj3dwWfHGDQe"
      },
      "source": [
        "# Frequencia de update\n",
        "INTERVALO_BATCH = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6YlTc7_GTUh"
      },
      "source": [
        "ssc = StreamingContext(sc, INTERVALO_BATCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8WRlunHGpAh"
      },
      "source": [
        "## Treinando o Classificador de Análise de Sentimento\n",
        "\n",
        "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineiração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle --> https://inclass.kaggle.com/c/si650winter11.\n",
        "\n",
        "Esse dataset contém 1,578,627 tweets classificados e cada linha linha é marcada como:\n",
        "\n",
        "1 para o sentimento positivo<br>\n",
        "0 para o sentimento negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDkZRf5PGu9R"
      },
      "source": [
        "# Lendo o arquivo de texto e criando um RDD em memória com Spark\n",
        "arquivo = sc.textFile(\"dataset_analise_sentimento.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dU3mvy_NFwH"
      },
      "source": [
        "# Removendo o cabeçalho\n",
        "header = arquivo.take(1)[0]\n",
        "dataset = arquivo.filter(lambda line: line != header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uEWDfmDNU08",
        "outputId": "bd5418fd-6159-4c79-9b58-343aeb524f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJl8HbRqNYPN"
      },
      "source": [
        "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
        "def get_row(line):\n",
        "  row = line.split(',')\n",
        "  sentimento = row[1]\n",
        "  tweet = row[3].strip()\n",
        "  translator = str.maketrans({key: None for key in string.punctuation})\n",
        "  tweet = tweet.translate(translator)\n",
        "  tweet = tweet.split(' ')\n",
        "  tweet_lower = []\n",
        "  for word in tweet:\n",
        "    tweet_lower.append(word.lower())\n",
        "  return (tweet_lower, sentimento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLura5TZOXMs"
      },
      "source": [
        "# Aplicação a função a cada linha do dataset\n",
        "dataset_treino = dataset.map(lambda line: get_row(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RouGNDv7OoKs"
      },
      "source": [
        "# Cria um objeto SentimentAnalyzer\n",
        "sentiment_analyzer = SentimentAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgMB1rkaO0J0",
        "outputId": "f55fdee3-b199-4aad-b465-04100544974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Certifique-se de ter espaço em disco - Aproximadamente 5GB\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETnXN9BwPFIn",
        "outputId": "4a7391c5-d28e-46fd-c5a1-f3a8d1a7c7a1",
        "colab": {
          "resources": {
            "http://localhost:8080/nltkdata.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url = 'nltkdata.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"nltkdata.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcFMtWJtPR1S"
      },
      "source": [
        "# Obtém a lista de stopwords em Inglês\n",
        "stopwords_all = []\n",
        "for word in stopwords.words('english'):\n",
        "  stopwords_all.append(word)\n",
        "  stopwords_all.append(word + '_NEG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_10NDv0P20-"
      },
      "source": [
        "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
        "dataset_treino_amostra = dataset_treino.take(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydM7_BPSLte"
      },
      "source": [
        "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
        "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofA_d93SgSi"
      },
      "source": [
        "# Cria um unigram (n-grama) e extrai as features\n",
        "# n-grama sequência contínua de itens, como sílabas, letras, palavras\n",
        "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
        "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
        "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoczJMWcTQTd",
        "outputId": "dcf1a8fa-c61b-4d9d-e757-94d5ea1ceb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.collections.LazyMap"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et4lTVRmTVVR",
        "outputId": "9178d691-31cb-40c2-a803-605acb1563c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Treinar o modelo\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentiment_analyzer.train(trainer, training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLN55VZUV0U"
      },
      "source": [
        "# Testa o classificador em algumas sentenças\n",
        "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
        "test_sentence2 = [(['tough','day','at','work','today']), '']\n",
        "test_sentence3 = [(['good','wonderful','amzing','awesome'], '')]\n",
        "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
        "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
        "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLwVqlzVE1V"
      },
      "source": [
        "# Autenticação do Twitter\n",
        "consumer_key = \"XXX\"\n",
        "consumer_secret = \"XXX\"\n",
        "access_token = \"XXX\"\n",
        "access_token_secret = \"XXX\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBVdBEOiXtej"
      },
      "source": [
        "# Especifica a URL termo de busca\n",
        "search_term = 'Trump'\n",
        "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
        "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMq1S7e1YGkc"
      },
      "source": [
        "# Criando o objeto de autenticação para o Twitter\n",
        "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHfV_PPLYSQk"
      },
      "source": [
        "rdd = ssc.sparkContext.parallelize([0])\n",
        "stream = ssc.queueStream([], default = rdd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga6GtcUuYdId",
        "outputId": "21c8035f-6e99-4d4c-efae-48a0dc35a3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(stream)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.streaming.dstream.DStream"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}