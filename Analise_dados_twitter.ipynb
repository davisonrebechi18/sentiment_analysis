{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analise_dados_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJoeN3e8_Gzk"
      },
      "source": [
        "# Análise de Dados do Twitter em Tempo Real com Python e Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRenfVnJCU8B"
      },
      "source": [
        "***Atenção***\n",
        "\n",
        "Utilize Java JDK 1.8 ou 11 e Apache Spark 2.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqSSGtwGDDMZ"
      },
      "source": [
        "## Instalação do Spark no Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt7ZS1_wGgjn",
        "outputId": "061713eb-751d-46f4-931f-96b296e4ec05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r                                                                               \r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N5-lspH_N8B"
      },
      "source": [
        "## Configuração de variáveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdOOq4twHN1K"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ACYMwhgHTYz",
        "outputId": "cf80a813-549a-4718-a184-aa73a7c5dce5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset_analise_sentimento.csv\tspark-2.3.1-bin-hadoop2.7.tgz\n",
            "sample_data\t\t\tspark-2.3.1-bin-hadoop2.7.tgz.1\n",
            "spark-2.3.1-bin-hadoop2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6t0uFzuR4X"
      },
      "source": [
        "## Spark Streaming Twitter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiScZwiND4ul",
        "outputId": "45a6eb67-8d1e-4fc4-c602-64e74ac9ecba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# pacotes necessário\n",
        "!pip install requests_oauthlib\n",
        "!pip install twython\n",
        "!pip install nltk\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests_oauthlib in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests_oauthlib) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests_oauthlib) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests_oauthlib) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests_oauthlib) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests_oauthlib) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests_oauthlib) (2.10)\n",
            "Requirement already satisfied: twython in /usr/local/lib/python3.6/dist-packages (3.8.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SE-5v6uEY1Y"
      },
      "source": [
        "# Módulos usados\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark import SparkContext\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from operator import add\n",
        "from time import gmtime, strftime\n",
        "import requests_oauthlib\n",
        "import requests\n",
        "import time\n",
        "import string\n",
        "import ast\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoNtvkuqFmgM"
      },
      "source": [
        "# Pacote NLTK\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.util import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj3dwWfHGDQe"
      },
      "source": [
        "# Frequencia de update\n",
        "INTERVALO_BATCH = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6YlTc7_GTUh"
      },
      "source": [
        "# Criando o StreamingContext\n",
        "sc =SparkContext.getOrCreate()\n",
        "ssc = StreamingContext(sc, INTERVALO_BATCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8WRlunHGpAh"
      },
      "source": [
        "## Treinando o Classificador de Análise de Sentimento\n",
        "\n",
        "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineiração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle --> https://inclass.kaggle.com/c/si650winter11.\n",
        "\n",
        "Esse dataset contém 1,578,627 tweets classificados e cada linha linha é marcada como:\n",
        "\n",
        "1 para o sentimento positivo<br>\n",
        "0 para o sentimento negativo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDkZRf5PGu9R"
      },
      "source": [
        "# Lendo o arquivo de texto e criando um RDD em memória com Spark\n",
        "arquivo = sc.textFile(\"dataset_analise_sentimento.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dU3mvy_NFwH"
      },
      "source": [
        "# Removendo o cabeçalho\n",
        "header = arquivo.take(1)[0]\n",
        "dataset = arquivo.filter(lambda line: line != header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uEWDfmDNU08",
        "outputId": "d88d093e-70bd-4446-d189-7cdab917f2c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJl8HbRqNYPN"
      },
      "source": [
        "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
        "def get_row(line):\n",
        "  row = line.split(',')\n",
        "  sentimento = row[1]\n",
        "  tweet = row[3].strip()\n",
        "  translator = str.maketrans({key: None for key in string.punctuation})\n",
        "  tweet = tweet.translate(translator)\n",
        "  tweet = tweet.split(' ')\n",
        "  tweet_lower = []\n",
        "  for word in tweet:\n",
        "    tweet_lower.append(word.lower())\n",
        "  return (tweet_lower, sentimento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLura5TZOXMs"
      },
      "source": [
        "# Aplicação a função a cada linha do dataset\n",
        "dataset_treino = dataset.map(lambda line: get_row(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RouGNDv7OoKs"
      },
      "source": [
        "# Cria um objeto SentimentAnalyzer\n",
        "sentiment_analyzer = SentimentAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgMB1rkaO0J0",
        "outputId": "28a2facd-5dd9-4837-e118-6e55dd7c6e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Certifique-se de ter espaço em disco - Aproximadamente 5GB\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETnXN9BwPFIn",
        "outputId": "634ee6ef-7493-421b-f7b0-8e8ec880a190",
        "colab": {
          "resources": {
            "http://localhost:8080/nltkdata.png": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url = 'nltkdata.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"nltkdata.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcFMtWJtPR1S"
      },
      "source": [
        "# Obtém a lista de stopwords em Inglês\n",
        "stopwords_all = []\n",
        "for word in stopwords.words('english'):\n",
        "  stopwords_all.append(word)\n",
        "  stopwords_all.append(word + '_NEG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_10NDv0P20-"
      },
      "source": [
        "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
        "dataset_treino_amostra = dataset_treino.take(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydM7_BPSLte"
      },
      "source": [
        "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
        "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofA_d93SgSi"
      },
      "source": [
        "# Cria um unigram (n-grama) e extrai as features\n",
        "# n-grama sequência contínua de itens, como sílabas, letras, palavras\n",
        "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
        "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
        "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoczJMWcTQTd",
        "outputId": "72c00874-9962-4fad-d0e0-8e0864ea0bb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.collections.LazyMap"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et4lTVRmTVVR",
        "outputId": "5db00f72-7d8e-43e4-bb07-9a81b706baa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Treinar o modelo\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentiment_analyzer.train(trainer, training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLN55VZUV0U"
      },
      "source": [
        "# Testa o classificador em algumas sentenças\n",
        "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
        "test_sentence2 = [(['tough','day','at','work','today']), '']\n",
        "test_sentence3 = [(['good','wonderful','amzing','awesome'], '')]\n",
        "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
        "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
        "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLwVqlzVE1V"
      },
      "source": [
        "# Autenticação do Twitter\n",
        "consumer_key = \"XXX\"\n",
        "consumer_secret = \"XXX\"\n",
        "access_token = \"XXX\"\n",
        "access_token_secret = \"XXX\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBVdBEOiXtej"
      },
      "source": [
        "# Especifica a URL termo de busca\n",
        "search_term = 'Trump'\n",
        "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
        "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track='+search_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMq1S7e1YGkc"
      },
      "source": [
        "# Criando o objeto de atutenticação para o Twitter\n",
        "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHfV_PPLYSQk"
      },
      "source": [
        "# Configurando o Stream\n",
        "rdd = ssc.sparkContext.parallelize([0])\n",
        "stream = ssc.queueStream([], default = rdd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga6GtcUuYdId",
        "outputId": "3ae40842-70a3-4285-ff72-766098db9f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Coleção de RDDs\n",
        "type(stream)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.streaming.dstream.DStream"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag-k9u1K-8Gb"
      },
      "source": [
        "# Total de tweets por update\n",
        "NUM_TWEETS = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPxPMrQK_E-f"
      },
      "source": [
        "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
        "def tfunc(t, rdd):\n",
        "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
        "\n",
        "def stream_twitter_data():\n",
        "  response = requests.get(filter_url, auth = auth, stream = True)\n",
        "  print(filter_url, response)\n",
        "  count = 0\n",
        "  for line in response.iter_lines():\n",
        "    try:\n",
        "      if count > NUM_TWEETS:\n",
        "        break\n",
        "      post = json.loads(line.decode('utf-8'))\n",
        "      contents = [post['text']]\n",
        "      count += 1\n",
        "      yield str(contents)\n",
        "    except:\n",
        "      result = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McsHwEfL_Mnf"
      },
      "source": [
        "stream = stream.transform(tfunc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47oAgR2AE2K"
      },
      "source": [
        "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_kmBBq1Aotd"
      },
      "source": [
        "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
        "def classifica_tweet(tweet):\n",
        "  sentence = [(tweet, '')]\n",
        "  test_set = sentiment_analyzer.apply_features(sentence)\n",
        "  print(tweet, classifier.classify(test_set[0][0]))\n",
        "  return(tweet, classifier.classify(test_set[0][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrjXntXpArP6"
      },
      "source": [
        "# Essa função retorna o texto do Twitter\n",
        "def get_tweet_text(rdd):\n",
        "  for line in rdd:\n",
        "    tweet = line.strip()\n",
        "    translator = str.maketrans({key: None for key in string.punctuation})\n",
        "    tweet = tweet.translate(translator)\n",
        "    tweet = tweet.split(' ')\n",
        "    tweet_lower = []\n",
        "    for word in tweet:\n",
        "      tweet_lower.append(word.lower())\n",
        "    return(classifica_tweet(tweet_lower))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYQizyzfAtSU"
      },
      "source": [
        "# Cria uma lista vazia para os resultados\n",
        "resultados = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0lgZKYtAyaJ"
      },
      "source": [
        "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
        "def output_rdd(rdd):\n",
        "  global resultados\n",
        "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
        "  counts = pairs.reduceByKey(add)\n",
        "  output = []\n",
        "  for count in counts.collect():\n",
        "    output.append(count)\n",
        "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
        "  resultados.append(result)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cYcyqlOA0-L"
      },
      "source": [
        "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
        "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RGPr8yzA3gv"
      },
      "source": [
        "# Start streaming\n",
        "ssc.start()\n",
        "# ssc.awaitTermination()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcEoVZ_jA78g"
      },
      "source": [
        "cont = True\n",
        "while cont:\n",
        "  if len(resultados) > 5:\n",
        "    cont = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUmpkZdpBFFP"
      },
      "source": [
        "# Grava os resultados\n",
        "rdd_save = 'sentiment_analysis'+time.strftime(\"%I%M%S\")\n",
        "resultados_rdd = sc.parallelize(resultados)\n",
        "resultados_rdd.saveAsTextFile(rdd_save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXDJk1yABYMI",
        "outputId": "4916bcc6-9bbe-4d6c-86d0-b91fd9817af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# Visualiza os resultados\n",
        "resultados_rdd.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['02:22:56', []],\n",
              " ['02:23:13', [('1', 132), ('0', 369)]],\n",
              " ['02:23:24', [('0', 373), ('1', 128)]],\n",
              " ['02:23:36', [('0', 367), ('1', 134)]],\n",
              " ['02:23:48', [('0', 389), ('1', 112)]],\n",
              " ['02:24:00', [('0', 355), ('1', 146)]],\n",
              " ['02:24:12', [('0', 374), ('1', 127)]],\n",
              " ['02:24:23', [('0', 382), ('1', 119)]],\n",
              " ['02:24:33', [('0', 383), ('1', 118)]],\n",
              " ['02:24:41', [('1', 117), ('0', 384)]],\n",
              " ['02:24:52', [('0', 381), ('1', 120)]],\n",
              " ['02:25:03', [('0', 367), ('1', 134)]],\n",
              " ['02:25:14', [('0', 365), ('1', 136)]],\n",
              " ['02:25:25', [('0', 368), ('1', 133)]],\n",
              " ['02:25:36', [('0', 371), ('1', 130)]],\n",
              " ['02:25:46', [('0', 393), ('1', 108)]],\n",
              " ['02:25:56', [('1', 123), ('0', 378)]],\n",
              " ['02:26:07', [('0', 370), ('1', 131)]],\n",
              " ['02:26:18', [('0', 377), ('1', 124)]],\n",
              " ['02:26:29', [('0', 377), ('1', 124)]],\n",
              " ['02:26:40', [('0', 372), ('1', 129)]],\n",
              " ['02:26:51', [('0', 381), ('1', 120)]],\n",
              " ['02:27:02', [('0', 391), ('1', 110)]],\n",
              " ['02:27:13', [('0', 364), ('1', 137)]],\n",
              " ['02:27:24', [('0', 379), ('1', 122)]],\n",
              " ['02:27:35', [('0', 366), ('1', 135)]],\n",
              " ['02:27:46', [('0', 382), ('1', 119)]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjYwOaubBeJs"
      },
      "source": [
        "# Finaliza o streaming\n",
        "ssc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe7jviWFD1Fv"
      },
      "source": [
        "Fim"
      ]
    }
  ]
}